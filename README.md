# Jam-gpt

An Experimental Reimplementation of large language model (LLM) architectures and design for research and development process. Its purpose is to create a simple and fast framework for training and fine-tuning medium-sized Generative Pretrained Transformers (GPT)

<p align="center">
<img src="https://user-images.githubusercontent.com/80915494/263127835-0509942a-0528-4471-96fa-8eda3d4f159c.jpeg" width="50%" height="50%" >
</p>

## Installation :

use pip and install it in your local site-packages directory

```bash
pip install git+https://github.com/Lokeshwaran-M/jam-gpt.git
```

## DOCS :

[Jam-gpt docs](./docs/jam-gpt.md) will give you the useage and explanation of the jam-gpt library

1 [ setup](./docs/jam-gpt.md#1-setup)  
2 [ Collecting data](./docs/jam-gpt.md#2-collecting-data)  
3 [ Tokenization](./docs/jam-gpt.md#3-tokenization)  
4 [ configurator]()  
5 [ Language Model (LM)]()  
6 [ Model generator]()


## credits :

kudos to [Andrej karpathy](https://github.com/karpathy) for his lectures on deep learning 