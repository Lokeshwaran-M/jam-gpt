{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size     :  0\n",
      "batch_size     :  16\n",
      "block_size     :  32\n",
      "max_iters      :  5000\n",
      "eval_interval  :  100\n",
      "learning_rate  :  0.001\n",
      "device         :  cuda\n",
      "eval_iters     :  200\n",
      "n_embd         :  64\n",
      "n_head         :  4\n",
      "n_layer        :  4\n",
      "dropout        :  0.0\n",
      "# -------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "from jam_gpt import Data, Tokenizer, config, lm, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time taken to load library :\n",
    "\n",
    "    time : 15.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  101\n",
      "parameters :  0.214373  M\n",
      "step 0: train loss 4.8401, val loss 4.8165\n",
      "step 100: train loss 3.0547, val loss 3.0512\n",
      "step 200: train loss 2.8194, val loss 2.8205\n",
      "step 300: train loss 2.7170, val loss 2.7103\n",
      "step 400: train loss 2.6313, val loss 2.6482\n",
      "step 500: train loss 2.5477, val loss 2.5833\n",
      "step 600: train loss 2.4494, val loss 2.5087\n",
      "step 700: train loss 2.3761, val loss 2.4409\n",
      "step 800: train loss 2.3006, val loss 2.3704\n",
      "step 900: train loss 2.2727, val loss 2.3089\n",
      "step 1000: train loss 2.2140, val loss 2.2710\n",
      "step 1100: train loss 2.1835, val loss 2.2199\n",
      "step 1200: train loss 2.1450, val loss 2.1923\n",
      "step 1300: train loss 2.1030, val loss 2.1597\n",
      "step 1400: train loss 2.0663, val loss 2.1175\n",
      "step 1500: train loss 2.0456, val loss 2.1424\n",
      "step 1600: train loss 2.0302, val loss 2.0658\n",
      "step 1700: train loss 1.9904, val loss 2.0483\n",
      "step 1800: train loss 1.9548, val loss 2.0511\n",
      "step 1900: train loss 1.9371, val loss 2.0237\n",
      "step 2000: train loss 1.9142, val loss 1.9805\n",
      "step 2100: train loss 1.8898, val loss 1.9831\n",
      "step 2200: train loss 1.8871, val loss 1.9790\n",
      "step 2300: train loss 1.8527, val loss 1.9592\n",
      "step 2400: train loss 1.8636, val loss 1.9121\n",
      "step 2500: train loss 1.8290, val loss 1.9266\n",
      "step 2600: train loss 1.8331, val loss 1.8865\n",
      "step 2700: train loss 1.8129, val loss 1.9087\n",
      "step 2800: train loss 1.7943, val loss 1.8916\n",
      "step 2900: train loss 1.7838, val loss 1.8794\n",
      "step 3000: train loss 1.7677, val loss 1.9027\n",
      "step 3100: train loss 1.7704, val loss 1.8713\n",
      "step 3200: train loss 1.7441, val loss 1.8633\n",
      "step 3300: train loss 1.7606, val loss 1.8585\n",
      "step 3400: train loss 1.7269, val loss 1.8600\n",
      "step 3500: train loss 1.7244, val loss 1.8291\n",
      "step 3600: train loss 1.7191, val loss 1.8333\n",
      "step 3700: train loss 1.7230, val loss 1.8472\n",
      "step 3800: train loss 1.7047, val loss 1.8149\n",
      "step 3900: train loss 1.6921, val loss 1.8188\n",
      "step 4000: train loss 1.6689, val loss 1.7952\n",
      "step 4100: train loss 1.6768, val loss 1.7994\n",
      "step 4200: train loss 1.6487, val loss 1.7940\n",
      "step 4300: train loss 1.6706, val loss 1.7661\n",
      "step 4400: train loss 1.6513, val loss 1.7917\n",
      "step 4500: train loss 1.6571, val loss 1.7886\n",
      "step 4600: train loss 1.6413, val loss 1.7803\n",
      "step 4700: train loss 1.6380, val loss 1.7909\n",
      "step 4800: train loss 1.6596, val loss 1.7620\n",
      "step 4900: train loss 1.6238, val loss 1.7738\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "\n",
    "\n",
    "path = \"./data-set/linuxsourcecodesnippets.txt\"\n",
    "\n",
    "# data collection\n",
    "data = Data.get(path)\n",
    "\n",
    "# tokanization\n",
    "model_name = \"md-t02-bglm\"\n",
    "tok.set_encoding(model_name, data)\n",
    "tok.get_encoding(model_name)\n",
    "enc_data = tok.encode(data)\n",
    "\n",
    "# setting parameters\n",
    "config.vocab_size = tok.n_vocab\n",
    "\n",
    "\n",
    "# model genration\n",
    "test_model = Model()\n",
    "test_model.set_model(lm.BigramLM())\n",
    "test_model.set_data(Data.train_test_split(enc_data))\n",
    "# test_model.load_model(model_name)\n",
    "test_model.optimize()\n",
    "test_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning detials : \n",
    "    step 4900 : train loss 1.6238, val loss 1.7738\n",
    "    traning time : 7m 54.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux engine and baddling to toabo\n",
      " *       ( :     0x, us change fon xecurrisided bfqnd-mased fanderister situmin[2] cack_current_shmsfm(q, q->easer_addr, t, max);\n",
      "\t\t\t\tgoto out_bfqq_struct_fined_dev(duw_ioccury(&bfqg->sizeof[IPTPRR_EXEGPTYP_TADUSHITOOON_FIT);\n",
      "}\n",
      "#defin] = PRC_OPTR_ERHTIC_PECHT);\n",
      "\t\tmm->pri\tmtg = current->usigned longfd;\n",
      "\tnatong long wait, emor bocks slizeofind matimd only the\n",
      "\t\t * request-is_init_compat_type(file, pall);\n",
      "\tboo alcook\", (blk_get_pol_aud(next_pos, &src);\n",
      "\tcryption_regionoug *re\n"
     ]
    }
   ],
   "source": [
    "pmt = tok.encode(\"linux engine\")\n",
    "print(tok.decode(test_model.generate(pmt)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
