{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size          :  0\n",
      "batch_size          :  32\n",
      "block_size          :  256\n",
      "max_iters           :  5000\n",
      "eval_interval       :  250\n",
      "learning_rate       :  0.001\n",
      "device              :  cuda\n",
      "eval_iters          :  200\n",
      "n_embd              :  384\n",
      "n_head              :  6\n",
      "n_layer             :  6\n",
      "dropout             :  0.2\n",
      "model_architecture  :  None\n",
      "# -------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "from jam_gpt import Data, Tokenizer, config, lm, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size          :  101\n",
      "batch_size          :  16\n",
      "block_size          :  32\n",
      "max_iters           :  5000\n",
      "eval_interval       :  100\n",
      "learning_rate       :  0.001\n",
      "device              :  cuda\n",
      "eval_iters          :  200\n",
      "n_embd              :  64\n",
      "n_head              :  4\n",
      "n_layer             :  4\n",
      "dropout             :  0.0\n",
      "model_architecture  :  lm.BigramLM\n",
      "# -------------------------------------#\n",
      "[101, 16, 32, 5000, 100, 0.001, 'cuda', 200, 64, 4, 4, 0.0, 'lm.BigramLM']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "\n",
    "\n",
    "# path = \"./data-set/linuxsourcecodesnippets.txt\"\n",
    "\n",
    "# # data collection\n",
    "# data = Data.get(path)\n",
    "\n",
    "# tokanization\n",
    "model_name = \"md-t02-bglm-rerun\"\n",
    "# tok.set_encoding(model_name, data)\n",
    "tok.get_encoding(model_name)\n",
    "# enc_data = tok.encode(data)\n",
    "\n",
    "# setting parameters\n",
    "# config.vocab_size = tok.n_vocab\n",
    "\n",
    "\n",
    "# model genration\n",
    "model = Model()\n",
    "# model.set_model(lm.BigramLM())\n",
    "# model.set_data(Data.train_test_split(enc_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  101\n",
      "parameters :  0.214373  M\n"
     ]
    }
   ],
   "source": [
    "model.load_model(model_name)\n",
    "# model.optimize()\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning detials : \n",
    "    step 4900: train loss 1.3497, val loss 1.6127\n",
    "    step 4900: train loss 1.5836, val loss 1.7533\n",
    "    step 4900: train loss 1.4185, val loss 1.6682\n",
    "    step 4900: train loss 1.3792, val loss 1.6416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* what is the linux file system command lstaign, Onlyss this atomically per itempt. Sause is not\n",
      "\t\t * drop end.\n",
      "\t\t\t new_bio_bio_cause(BLK_SECINIT(mmdelk_current_ffd(fmt, SEC, aead-addr, thimum);\n",
      " *       Meter/sending seforminal12/init, this name texply in\n",
      " * the procsronally to\n",
      " * base default can orcharry and, as only all finally rismet and propirs.\n",
      " */\n",
      "void bfq_addrt_note(void);\n",
      "\n",
      "/*\n",
      " * The write it initializy in rate virariations ->samplete {\n",
      "\t\tdong *from;\n",
      "\n",
      "\t\tif (wbc->alg->bacck_lzsib(iop)) {\n",
      "\t\t\tinfo->dun_info->cval_info->nsigned || \n"
     ]
    }
   ],
   "source": [
    "pmt = tok.encode(\"\"\"* what is the linux file system command ls\"\"\")\n",
    "print(tok.decode(model.generate(pmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_model(\"md-t02-bglm-rerun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size          :  0\n",
      "batch_size          :  32\n",
      "block_size          :  256\n",
      "max_iters           :  5000\n",
      "eval_interval       :  250\n",
      "learning_rate       :  0.001\n",
      "device              :  cuda\n",
      "eval_iters          :  200\n",
      "n_embd              :  384\n",
      "n_head              :  6\n",
      "n_layer             :  6\n",
      "dropout             :  0.2\n",
      "model_architecture  :  None\n",
      "# -------------------------------------#\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from jam_gpt.data import Data\n",
    "\n",
    "\n",
    "d = Data.formater(\n",
    "\"\"\"\n",
    "4927\n",
    "\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from . import config\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        [self.vocab_size, self.batch_size, self.block_size, self.max_iters, self.eval_interval, self.learning_rate,\n",
    "            self.device, self.eval_iters, self.n_embd, self.n_head, self.n_layer, self.dropout] = config.pass_args()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "    def set_parameters(self, args: list):\n",
    "        [self.vocab_size, self.batch_size, self.block_size, self.max_iters, self.eval_interval, self.learning_rate,\n",
    "            self.device, self.eval_iters, self.n_embd, self.n_head, self.n_layer, self.dropout] = args\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [self.vocab_size, self.batch_size, self.block_size, self.max_iters, self.eval_interval, self.learning_rate, self.device, self.eval_iters, self.n_embd, self.n_head, self.n_layer, self.dropout]\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.m = self.model.to(self.device)\n",
    "        # print the number of parameters in the model\n",
    "        print(\"vocab size : \", self.vocab_s\n",
    "\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "write a function that takes three string context prompt output as input and returns output as json object\n",
    "{context: \"context string\" ,prompt: \"prompt sting\",output: \"output string \" }\n",
    "View other drafts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sure, here is a Python function that takes three strings (context, prompt, and output) as input and returns an output JSON object:\n",
    "\n",
    "Python\n",
    "import json\n",
    "\n",
    "def create_json_object(context, prompt, output):\n",
    "  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size          :  0\n",
      "batch_size          :  32\n",
      "block_size          :  256\n",
      "max_iters           :  5000\n",
      "eval_interval       :  250\n",
      "learning_rate       :  0.001\n",
      "device              :  cuda\n",
      "eval_iters          :  200\n",
      "n_embd              :  384\n",
      "n_head              :  6\n",
      "n_layer             :  6\n",
      "dropout             :  0.2\n",
      "model_architecture  :  None\n",
      "# -------------------------------------#\n",
      "# -------------------------------------#\n",
      "# hyperparameters\n",
      "vocab_size          :  101\n",
      "batch_size          :  16\n",
      "block_size          :  32\n",
      "max_iters           :  5000\n",
      "eval_interval       :  100\n",
      "learning_rate       :  0.001\n",
      "device              :  cuda\n",
      "eval_iters          :  50000\n",
      "n_embd              :  64\n",
      "n_head              :  4\n",
      "n_layer             :  4\n",
      "dropout             :  0.0\n",
      "model_architecture  :  lm.BigramLM\n",
      "# -------------------------------------#\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101, 16, 32, 5000, 100, 0.001, 'cuda', 50000, 64, 4, 4, 0.0, 'lm.BigramLM']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_args()\n",
    "\n",
    "config.retrive(\"md-t02-bglm\")\n",
    "\n",
    "config.eval_iters = 50000\n",
    "\n",
    "\n",
    "config.get_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
